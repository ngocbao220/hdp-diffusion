# üéØ HIERARCHICAL BLOCK DIFFUSION - COMPLETE IMPLEMENTATION

## üìã T·ªïng quan nhanh

ƒê√¢y l√† implementation ƒë·∫ßy ƒë·ªß c·ªßa **Hierarchical Discrete Diffusion Model** v·ªõi ki·∫øn tr√∫c **Plan-then-Generate** d·ª±a tr√™n BD3-LM (Block Discrete Denoising Diffusion Language Models).

**3 ƒëi·ªÉm c·ªët l√µi:**
1. ‚úÖ **Hierarchical Attention Mask** - Plan kh√¥ng th·ªÉ nh√¨n th·∫•y Execution
2. ‚úÖ **Structured Data Format** - [Question | Plan | Execution]
3. ‚úÖ **Simplified Generation** - Fixed length, no arbitrary-length

---

## üìö T√†i li·ªáu ƒë·∫ßy ƒë·ªß (ƒê·ªçc theo th·ª© t·ª±)

### üöÄ B·∫Øt ƒë·∫ßu nhanh
- **[QUICKSTART.md](QUICKSTART.md)** ‚≠ê B·∫ÆT ƒê·∫¶U T·ª™ ƒê√ÇY
  - 3 b∆∞·ªõc ƒë·ªÉ ch·∫°y th·ª≠
  - Quick reference commands
  - Troubleshooting nhanh
  - **Th·ªùi gian ƒë·ªçc: 5 ph√∫t**

### üìñ Documentation ch√≠nh
- **[HIERARCHICAL_README.md](HIERARCHICAL_README.md)** (Ti·∫øng Vi·ªát)
  - Gi·∫£i th√≠ch architecture chi ti·∫øt
  - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ƒë·∫ßy ƒë·ªß
  - Customization guide
  - Examples v√† best practices
  - **Th·ªùi gian ƒë·ªçc: 20 ph√∫t**

### üîß Implementation details
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)**
  - Chi ti·∫øt t·∫•t c·∫£ thay ƒë·ªïi
  - Code structure
  - Integration points
  - **Th·ªùi gian ƒë·ªçc: 15 ph√∫t**

### üìä Visual reference
- **[architecture_diagram.py](architecture_diagram.py)**
  - ASCII diagrams
  - Attention flow visualization
  - **Run ƒë·ªÉ xem:** `python architecture_diagram.py`

### üéì GSM8K Math Reasoning (NEW!)
- **[GSM8K_README.md](GSM8K_README.md)** ‚≠ê GSM8K Quick Start
  - Complete pipeline for math reasoning
  - Plan generation with vLLM + Llama-3
  - Training & evaluation on 7.5k problems
  - **Th·ªùi gian ƒë·ªçc: 10 ph√∫t**
- **[GSM8K_TRAINING_GUIDE.md](GSM8K_TRAINING_GUIDE.md)** 
  - Detailed step-by-step guide
  - Experiments for paper
  - Troubleshooting & tips
  - **Th·ªùi gian ƒë·ªçc: 20 ph√∫t**
- **[GSM8K_SETUP_SUMMARY.md](GSM8K_SETUP_SUMMARY.md)**
  - Quick setup summary
  - File structure & deliverables
  - **Th·ªùi gian ƒë·ªçc: 5 ph√∫t**

### ‚úÖ Final summary
- **[FINAL_SUMMARY.md](FINAL_SUMMARY.md)**
  - Verification checklist
  - Comparison v·ªõi baseline
  - Next steps
  - **Th·ªùi gian ƒë·ªçc: 10 ph√∫t**

---

## üíª Code Files

### Core Implementation
| File | Ch·ª©c nƒÉng | Status |
|------|-----------|--------|
| **[models/hierarchical_mask.py](models/hierarchical_mask.py)** | Attention mask ph√¢n t·∫ßng | ‚úÖ Complete |
| **[hierarchical_dataloader.py](hierarchical_dataloader.py)** | Data preprocessing | ‚úÖ Complete |
| **[test_hierarchical_mask.py](test_hierarchical_mask.py)** | Unit tests & verification | ‚úÖ Complete |

### Configuration
| File | Ch·ª©c nƒÉng | Status |
|------|-----------|--------|
| **[configs/algo/hierarchical.yaml](configs/algo/hierarchical.yaml)** | Hierarchical config | ‚úÖ Complete |
| **[scripts/train/train_hierarchical_bd3lm.sh](scripts/train/train_hierarchical_bd3lm.sh)** | Training script | ‚úÖ Complete |

### Modified Files
| File | Thay ƒë·ªïi | Line |
|------|----------|------|
| **[models/dit.py](models/dit.py)** | Added `hierarchical_config` param | ~710 |

---

## ‚ö° Quick Commands

### Test mask (kh√¥ng c·∫ßn GPU)
```bash
python test_hierarchical_mask.py
```
**Output mong ƒë·ª£i:**
- ‚úÖ All tests passed
- üìä File `hierarchical_mask_test.png` ƒë∆∞·ª£c t·∫°o

### Visualize architecture
```bash
python architecture_diagram.py
```
**Output:** ASCII diagrams showing attention flow

### Train v·ªõi data test
```bash
# Edit ƒë·ªÉ gi·∫£m steps (test nhanh)
vim scripts/train/train_hierarchical_bd3lm.sh

# Run
sbatch scripts/train/train_hierarchical_bd3lm.sh
```

### Train tr·ª±c ti·∫øp
```bash
python main.py \
    mode=train \
    model=tiny \
    algo=bd3lm \
    block_size=16 \
    training.max_steps=1000 \
    training.hierarchical.enabled=true \
    training.hierarchical.question_len=256 \
    training.hierarchical.plan_len=256 \
    training.hierarchical.exec_len=512
```

---

## üéØ C·∫•u tr√∫c Hierarchical Mask

### Sequence Structure
```
[Question | Plan_xt | Plan_x0 | Exec_xt | Exec_x0]
   256    |   256    |   256   |   512   |   512   = 1792 tokens
```

### Attention Rules
```
‚úÖ Question ‚Üí Question (full attention)
‚úÖ Plan ‚Üí Question (can see input)
‚úÖ Plan ‚Üí Plan (block diffusion)
‚ùå Plan ‚Üí Execution (BLOCKED!)
‚úÖ Execution ‚Üí Question (can see input)
‚úÖ Execution ‚Üí Plan (can see high-level plan)
‚úÖ Execution ‚Üí Execution (block diffusion)
```

**ƒêi·ªÉm quan tr·ªçng:** Plan KH√îNG th·ªÉ nh√¨n th·∫•y Execution ‚Üí Gi·ªØ t√≠nh nh√¢n qu·∫£

### Verification
```python
from models.hierarchical_mask import create_hierarchical_mask

mask = create_hierarchical_mask(
    seqlen=1024, block_size=16,
    question_len=256, plan_len=256, exec_len=512
)

# Verify Plan cannot see Execution
plan_to_exec = mask[256:768, 768:]  # Plan tokens ‚Üí Exec tokens
assert not plan_to_exec.any(), "Plan should NOT see Execution!"
print("‚úÖ Causal constraint verified!")
```

---

## üì¶ Data Format

### Option A: Structured Data
```json
[
  {
    "question": "What is the capital of France?",
    "plan": "I need to recall European geography and capitals.",
    "execution": "Paris is the capital and largest city of France..."
  }
]
```

**Load:**
```python
from hierarchical_dataloader import load_reasoning_dataset

dataset = load_reasoning_dataset(
    dataset_path='data.json',
    tokenizer=tokenizer,
    question_len=256,
    plan_len=256,
    exec_len=512
)
```

### Option B: Auto-split
```python
from hierarchical_dataloader import HierarchicalDataCollator

collator = HierarchicalDataCollator(tokenizer, 256, 256, 512)
batch = collator([{'text': 'Your text here...'}])
# Auto-splits: 25% Q, 25% P, 50% E
```

### Option C: Custom Parser
```python
# In hierarchical_dataloader.py
def process_example(example):
    text = example['text']
    
    # TODO: Your custom logic
    question = your_extract_question(text)
    plan = your_extract_plan(text)
    execution = your_extract_execution(text)
    
    return {'question': ..., 'plan': ..., 'execution': ...}
```

---

## üîß Configuration Options

### Sequence Lengths (Adjustable)
```yaml
hierarchical:
  question_len: 256  # Question/context tokens
  plan_len: 256      # High-level plan tokens
  exec_len: 512      # Detailed execution tokens
  total_len: 1024    # Sum of above
```

### Block Size (Speed vs Quality)
```yaml
block_size: 16  # Options: 1, 4, 8, 16, 32, 64, 1024
```

**Trade-off:**
- `block_size=1`: Slowest, highest quality (Autoregressive)
- `block_size=16`: Balanced (Recommended)
- `block_size=1024`: Fastest, lower quality (Full diffusion)

### Training Settings
```yaml
training:
  max_steps: 100000
  warmup_steps: 10000
  batch_size: 64
  learning_rate: 5e-4
  ema: 0.9999
```

### Features Disabled (Simplified)
```yaml
sampling:
  var_length: false        # No variable-length
  arbitrary_length: false  # Fixed 1024 tokens
  first_hitting: true      # Faster sampling
  kv_cache: true          # Speed optimization
```

---

## ‚úÖ Verification Checklist

### Pre-training
- [ ] Run: `python test_hierarchical_mask.py`
  - [ ] All tests passed
  - [ ] Visualization looks correct
- [ ] Check data format
  - [ ] Print 3-5 samples
  - [ ] Verify [Q, P, E] structure
- [ ] Review config
  - [ ] Lengths: 256/256/512
  - [ ] Block size: 16
  - [ ] LR, warmup, etc.

### During training
- [ ] Loss decreases (first 100 steps)
- [ ] No NaN/Inf values
- [ ] GPU utilization good (>80%)
- [ ] Checkpoints saving correctly

### Post-training
- [ ] Validate loss better than baseline
- [ ] Generate samples
  - [ ] Has hierarchical structure?
  - [ ] Plan makes sense?
  - [ ] Execution follows plan?
- [ ] Compare with AR/MDLM/SEDD

---

## üêõ Common Issues

| Problem | Quick Fix |
|---------|-----------|
| "Mask dimensions don't match" | Check: `total = q_len + p_len*2 + e_len*2` |
| CUDA OOM | Reduce `batch_size` or use `model=tiny` |
| Loss not decreasing | Verify mask with test, reduce LR |
| "torch not found" | Install: `pip install -r requirements.txt` |
| Tests failing | Check Python/PyTorch version |

**Chi ti·∫øt:** See Troubleshooting section in [HIERARCHICAL_README.md](HIERARCHICAL_README.md)

---

## üöÄ Next Steps

### Ngay b√¢y gi·ªù (5 ph√∫t)
1. Read [QUICKSTART.md](QUICKSTART.md)
2. Run `python test_hierarchical_mask.py`
3. Check visualization: `hierarchical_mask_test.png`

### H√¥m nay (1-2 gi·ªù)
4. Prepare 100-1000 examples of your data
5. Implement custom parser (if needed)
6. Run small training test (1000 steps)

### Tu·∫ßn n√†y (2-3 ng√†y)
7. Full training (100K steps)
8. Evaluate on test set
9. Compare with baselines

### Th√°ng n√†y (1-2 tu·∫ßn)
10. Tune hyperparameters
11. Try different block sizes
12. Write paper/report

---

## üìä Expected Results

### Training Metrics
- **Initial loss:** ~8-10 (random)
- **After 1K steps:** ~6-7
- **After 10K steps:** ~5-6
- **After 100K steps:** ~4-5 (depends on data)

### Generation Quality
- **Plan coherence:** Should be high-level, logical
- **Execution detail:** Should follow plan structure
- **Causality:** Plan should not "leak" execution details

### Comparison
| Model | Perplexity | Speed | Hierarchical |
|-------|-----------|-------|--------------|
| AR | Best | Slow | ‚ùå |
| MDLM | Good | Fast | ‚ùå |
| BD3-LM | Good | Medium | ‚ùå |
| **Hier-BD3-LM** | Good | Medium | ‚úÖ |

---

## üìö References

### Papers
- **BD3-LM:** Block Diffusion (ICLR 2025)
- **Appendix B.6, B.7:** Hierarchical architecture
- **Figure 4:** Attention mask visualization

### Code
- **Original repo:** kuleshov-group/bd3lm
- **HuggingFace:** kuleshov-group/bd3-lms

### Citation
```bibtex
@inproceedings{arriola2025block,
  title={Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models},
  author={Arriola, Marianne and Gokaslan, Aaron and ...},
  booktitle={ICLR},
  year={2025}
}
```

---

## üí° Tips & Tricks

### For faster debugging
```bash
# Use tiny model + small data
python main.py model=tiny training.max_steps=100 loader.batch_size=8
```

### For better quality
```bash
# Increase warmup, use EMA
training.warmup_steps=20000 training.ema=0.9999
```

### For faster inference
```bash
# Enable caching, use first-hitting
sampling.kv_cache=true sampling.first_hitting=true
```

### For customization
1. Edit mask: `models/hierarchical_mask.py`
2. Edit data: `hierarchical_dataloader.py`
3. Edit config: `configs/algo/hierarchical.yaml`

---

## üìû Getting Help

### Documentation
1. **[QUICKSTART.md](QUICKSTART.md)** - Quick commands
2. **[HIERARCHICAL_README.md](HIERARCHICAL_README.md)** - Full guide
3. **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)** - Details

### Testing
```bash
python test_hierarchical_mask.py  # Verify implementation
python architecture_diagram.py    # View diagrams
```

### Common questions
- **How do I know mask is correct?** ‚Üí Run tests, check visualization
- **My data format is different?** ‚Üí Edit `process_example()` in dataloader
- **Training too slow?** ‚Üí Reduce batch_size, use smaller model
- **How to evaluate?** ‚Üí Check perplexity, generate samples, manual inspection

---

## ‚ú® Summary

**‚úÖ Ho√†n th√†nh:**
- Hierarchical attention mask (Plan ‚Üí Execution blocked)
- Data collator for [Question, Plan, Execution]
- Simplified generation (fixed length)
- Full documentation & tests

**üì¶ Deliverables:**
- 9 new files (code + docs)
- 1 modified file (dit.py)
- Working training script
- Comprehensive tests

**üéØ Ready for:**
- Small-scale testing
- Custom data integration
- Full training & evaluation

**üöÄ Start here:** [QUICKSTART.md](QUICKSTART.md)

---

**Last updated:** 2026-01-01  
**Status:** ‚úÖ Complete & Ready to Use  
**Contact:** See documentation for details

# GSM8K Hierarchical Training Guide

## üéØ M·ª•c ti√™u

Train Hierarchical BD3-LM tr√™n GSM8K ƒë·ªÉ ch·ª©ng minh:
- **H-Module (Plan)**: H·ªçc t∆∞ duy to√°n h·ªçc tr·ª´u t∆∞·ª£ng
- **L-Module (Execution)**: H·ªçc c√°ch t√≠nh to√°n c·ª• th·ªÉ v·ªõi s·ªë

## üìã Pipeline T·ªïng quan

```
GSM8K Raw Data (7.5k examples)
    ‚Üì
[1] Generate Plans v·ªõi Llama-3-8B
    ‚Üì
Hierarchical Format: [Question | Plan | Execution]
    ‚Üì
[2] Train Hierarchical BD3-LM
    ‚Üì
Evaluate reasoning quality
```

---

## üöÄ B∆Ø·ªöC 1: Generate Plans cho GSM8K

### 1.1 Chu·∫©n b·ªã m√¥i tr∆∞·ªùng

```bash
# Install vLLM (n·∫øu ch∆∞a c√≥)
pip install vllm

# Create directories
mkdir -p data/gsm8k logs scripts/data_prep
```

### 1.2 Ch·∫°y plan generation tr√™n H200

**Script:** `scripts/data_prep/run_gsm8k_plan_generation.sh`

```bash
# Test v·ªõi 100 examples tr∆∞·ªõc
sbatch scripts/data_prep/run_gsm8k_plan_generation.sh

# Ho·∫∑c test nhanh (kh√¥ng qua SLURM):
python scripts/data_prep/generate_gsm8k_plans.py \
    --model meta-llama/Meta-Llama-3-8B-Instruct \
    --split train \
    --batch_size 256 \
    --output_path data/gsm8k/gsm8k_hierarchical_train.json \
    --num_examples 100  # Test v·ªõi 100 examples
```

**Th·ªùi gian ∆∞·ªõc t√≠nh:**
- 100 examples: ~2 ph√∫t
- 7,500 examples (full train): ~30-40 ph√∫t v·ªõi H200
- 1,300 examples (test set): ~5-10 ph√∫t

### 1.3 Ki·ªÉm tra output

```bash
# Check file ƒë∆∞·ª£c t·∫°o
ls -lh data/gsm8k/

# View statistics
jq 'length' data/gsm8k/gsm8k_hierarchical_train.json

# View first example
jq '.[0]' data/gsm8k/gsm8k_hierarchical_train.json
```

**Expected output format:**
```json
{
  "id": "gsm8k_train_0",
  "question": "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?",
  "plan": "Calculate daily egg production. Determine total consumption from breakfast and baking. Subtract consumption from production to find remainder. Multiply remainder by unit price to get daily revenue.",
  "execution": "Janet gets 16 eggs per day. She eats 3 for breakfast and uses 4 for muffins, totaling 7 eggs. This leaves 16 - 7 = 9 eggs to sell. She sells them for $2 each, making 9 * 2 = $18 per day.",
  "answer_numerical": "18"
}
```

### 1.4 Analyze token lengths

```bash
# Analyze ƒë·ªÉ x√°c ƒë·ªãnh optimal max_len
python gsm8k_dataloader.py \
    --data_path data/gsm8k/gsm8k_hierarchical_train.json \
    --tokenizer gpt2 \
    --analyze
```

**Expected output:**
```
GSM8K Token Length Analysis
============================================================
Number of examples: 7473

Question:
  Mean: 65.3
  Median: 62.0
  95th percentile: 105.0

Plan:
  Mean: 42.1
  Median: 38.0
  95th percentile: 75.0

Execution:
  Mean: 98.7
  Median: 89.0
  95th percentile: 165.0

Recommended max_len settings (95th percentile):
  question_max_len: 128
  plan_max_len: 128
  exec_max_len: 256
  total: 512
```

---

## üîß B∆Ø·ªöC 2: Training Hierarchical BD3-LM

### 2.1 Verify data v√† mask

```bash
# Test dataloader
python gsm8k_dataloader.py \
    --data_path data/gsm8k/gsm8k_hierarchical_train.json \
    --tokenizer gpt2

# Test hierarchical mask
python test_hierarchical_mask.py
```

### 2.2 Training configuration

**Default settings** (trong `scripts/train/train_gsm8k_hierarchical.sh`):

```bash
QUESTION_LEN=128   # Short math problems
PLAN_LEN=128       # Concise reasoning steps
EXEC_LEN=256       # Detailed calculations
BLOCK_SIZE=16      # Balance speed/quality

MAX_STEPS=50000    # ~6.7 epochs over 7.5k examples
BATCH_SIZE=32
LR=3e-4
```

### 2.3 Run training

```bash
# Full training
sbatch scripts/train/train_gsm8k_hierarchical.sh

# Or test v·ªõi small run (1000 steps)
python main.py \
    mode=train \
    model=tiny \
    model.length=512 \
    algo=bd3lm \
    block_size=16 \
    training.max_steps=1000 \
    training.hierarchical.enabled=true \
    training.hierarchical.question_len=128 \
    training.hierarchical.plan_len=128 \
    training.hierarchical.exec_len=256 \
    data.name=gsm8k \
    data.train_path=data/gsm8k/gsm8k_hierarchical_train.json \
    data.test_path=data/gsm8k/gsm8k_hierarchical_test.json
```

### 2.4 Monitor training

```bash
# Watch logs
tail -f logs/gsm8k_hier_bd3lm_*.out

# Check tensorboard (if available)
tensorboard --logdir outputs/gsm8k_hierarchical_bd3lm_bs16
```

**Expected metrics:**
- Initial loss: ~8-9
- After 1K steps: ~6-7
- After 10K steps: ~5-6
- After 50K steps: ~4-5

---

## üìä B∆Ø·ªöC 3: Evaluation & Analysis

### 3.1 Generate samples

```bash
python main.py \
    mode=sample_eval \
    model=small \
    algo=bd3lm \
    eval.checkpoint_path=outputs/gsm8k_hierarchical_bd3lm_bs16/best.ckpt \
    sampling.num_sample_batches=100 \
    training.hierarchical.enabled=true
```

### 3.2 Analyze Plan quality

T·∫°o script ƒë·ªÉ evaluate:

```python
# analyze_plans.py
import json

with open('generated_samples.json') as f:
    samples = json.load(f)

# Check if Plans are abstract (no specific numbers)
for sample in samples[:10]:
    plan = sample['plan']
    # Count numbers in plan
    import re
    numbers = re.findall(r'\d+', plan)
    print(f"Plan: {plan}")
    print(f"Contains {len(numbers)} numbers")
    print()
```

### 3.3 Compare v·ªõi baseline

**Metrics ƒë·ªÉ so s√°nh:**

1. **Perplexity**: NELBO tr√™n test set
2. **Accuracy**: % correct answers (n·∫øu c√≥ ground truth)
3. **Plan abstraction**: S·ªë l∆∞·ª£ng numbers trong plan (c√†ng √≠t c√†ng t·ªët)
4. **Execution detail**: Numbers trong execution (c√†ng nhi·ªÅu c√†ng t·ªët)

---

## üé® Prompt Engineering Tips

### N·∫øu Plans c√≥ qu√° nhi·ªÅu s·ªë c·ª• th·ªÉ:

**C·∫£i thi·ªán prompt:**

```python
PLAN_EXTRACTION_PROMPT = """...

**CRITICAL**: Replace ALL specific numbers with abstract terms:
- Instead of "5 apples" ‚Üí "initial quantity"
- Instead of "add 3" ‚Üí "add given amount"
- Instead of "multiply by 2" ‚Üí "apply multiplication"
- Instead of "$10" ‚Üí "unit price"

Your plan should read like a recipe that works for ANY numbers.

..."""
```

### N·∫øu Plans qu√° ng·∫Øn ho·∫∑c kh√¥ng r√µ:

```python
# TƒÉng temperature
TEMPERATURE=0.8  # instead of 0.7

# TƒÉng max_tokens
MAX_TOKENS=384  # instead of 256
```

### N·∫øu Plans copy t·ª´ execution:

```python
# Th√™m v√†o prompt:
"Do NOT copy the execution steps. Create NEW abstract reasoning steps."
```

---

## üî¨ Experiments ƒë·ªÉ ch·ª©ng minh trong Paper

### Experiment 1: Plan Abstraction

**Gi·∫£ thuy·∫øt:** Plan module h·ªçc ƒë∆∞·ª£c abstract reasoning

**Metrics:**
- Number density trong Plan vs Execution
- Plan similarity across different numbers (edit distance)

**Code:**
```python
import re

def count_numbers(text):
    return len(re.findall(r'\d+', text))

plan_numbers = [count_numbers(x['plan']) for x in samples]
exec_numbers = [count_numbers(x['execution']) for x in samples]

print(f"Avg numbers in Plan: {np.mean(plan_numbers):.1f}")
print(f"Avg numbers in Exec: {np.mean(exec_numbers):.1f}")
```

### Experiment 2: Transfer Learning

**Gi·∫£ thuy·∫øt:** Pre-trained Plan module c√≥ th·ªÉ transfer sang problems kh√°c

**Setup:**
1. Train tr√™n GSM8K
2. Freeze Plan module
3. Fine-tune Execution module tr√™n SVAMP ho·∫∑c dataset kh√°c
4. So s√°nh v·ªõi baseline (kh√¥ng freeze)

### Experiment 3: Ablation Study

**So s√°nh:**
1. Hierarchical (Plan + Execution)
2. Flat (no Plan, ch·ªâ c√≥ Question + Execution)
3. Baseline AR model

**Metrics:** Perplexity, accuracy, inference speed

---

## üìù Expected Results (V√≠ d·ª•)

### Sample Generated Output:

**Question:**
```
Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
```

**Generated Plan:**
```
Identify the quantity sold in the first period. Calculate the quantity for the second period as half of the first. Sum both quantities to get total.
```

**Generated Execution:**
```
Natalia sold 48 clips in April. In May, she sold half of that: 48 / 2 = 24 clips. Total clips sold: 48 + 24 = 72 clips.
```

**Answer:** 72 ‚úì

### Analysis:

‚úÖ **Plan is abstract:** No specific numbers (48, 24, 72)
‚úÖ **Plan is reusable:** Works for any "sell X then half" problem
‚úÖ **Execution has details:** All calculations present
‚úÖ **Answer is correct:** Matches ground truth

---

## üêõ Troubleshooting

### Problem: vLLM installation fails

```bash
# Try with specific version
pip install vllm==0.4.0

# Or use Docker
docker pull vllm/vllm-openai:latest
```

### Problem: Plans are too similar to Execution

**Solution 1:** Improve prompt (add more emphasis on abstraction)

**Solution 2:** Use few-shot examples in prompt:
```python
PLAN_EXTRACTION_PROMPT = """...

Here are 3 good examples:
1. Question: "Add 5 and 3"
   Bad Plan: "Add 5 and 3 to get 8"
   Good Plan: "Apply addition operation to given numbers"

2. Question: "Multiply 4 by 7"
   Bad Plan: "Multiply 4 by 7 to get 28"
   Good Plan: "Use multiplication with provided operands"

..."""
```

### Problem: Training loss not decreasing

1. Check mask visualization
2. Verify data format (print samples)
3. Reduce learning rate: `LR=1e-4`
4. Increase warmup: `WARMUP_STEPS=10000`

### Problem: GPU OOM

```bash
# Reduce batch size
BATCH_SIZE=16

# Use smaller model
MODEL_SIZE=tiny

# Reduce sequence length
QUESTION_LEN=96
PLAN_LEN=96
EXEC_LEN=192
```

---

## üìä Checklist

### Data Preparation
- [ ] Generate plans: `sbatch scripts/data_prep/run_gsm8k_plan_generation.sh`
- [ ] Verify JSON format: `jq '.[0]' data/gsm8k/gsm8k_hierarchical_train.json`
- [ ] Analyze lengths: `python gsm8k_dataloader.py --analyze`
- [ ] Check plan quality: Manually inspect 10-20 examples

### Training
- [ ] Test dataloader: `python gsm8k_dataloader.py`
- [ ] Test mask: `python test_hierarchical_mask.py`
- [ ] Small training test (1K steps): Check loss decreases
- [ ] Full training: Monitor for 50K steps
- [ ] Save checkpoints: Check `outputs/gsm8k_hierarchical_bd3lm_bs16/`

### Evaluation
- [ ] Generate samples from checkpoint
- [ ] Calculate metrics (perplexity, accuracy)
- [ ] Analyze plan abstraction
- [ ] Compare with baselines
- [ ] Prepare results for paper

---

## üéØ Next Steps

### Immediate (Today)
1. Run plan generation: `sbatch scripts/data_prep/run_gsm8k_plan_generation.sh`
2. Check output quality
3. Start training test (1K steps)

### This Week
4. Full training (50K steps)
5. Generate evaluation samples
6. Analyze plan quality

### This Month
7. Run ablation studies
8. Compare with baselines
9. Write paper section

---

## üìö References

- **GSM8K Dataset:** https://huggingface.co/datasets/gsm8k
- **vLLM Documentation:** https://docs.vllm.ai/
- **Llama-3-8B:** https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct

---

**Summary:** Pipeline ƒë·ªÉ train hierarchical BD3-LM tr√™n GSM8K ƒë√£ s·∫µn s√†ng. B·∫Øt ƒë·∫ßu b·∫±ng vi·ªác generate plans, sau ƒë√≥ training v√† evaluation! üöÄ

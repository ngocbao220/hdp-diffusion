#!/usr/bin/env python3
"""
Kiá»ƒm tra chi tiáº¿t baseline GSM8K training data format
"""

from transformers import AutoTokenizer
import datasets

print("="*80)
print("ğŸ” KIá»‚M TRA BASELINE GSM8K DATA FORMAT")
print("="*80)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# Load GSM8K dataset tá»« HuggingFace
print("\n1ï¸âƒ£ Loading GSM8K tá»« HuggingFace...")
dataset = datasets.load_dataset('gsm8k', 'main', split='train[:5]')

print(f"\nğŸ“Š Dataset info:")
print(f"   Sá»‘ samples: {len(dataset)}")
print(f"   Keys: {list(dataset[0].keys())}")

# Xem sample Ä‘áº§u tiÃªn
print(f"\n{'='*80}")
print("ğŸ“ SAMPLE 1 - RAW DATA:")
print(f"{'='*80}")
sample = dataset[0]
print(f"\nâ“ Question:")
print(f"   {sample['question']}")
print(f"\nâœ… Answer:")
print(f"   {sample['answer']}")

# Xem baseline format trong dataloader.py
print(f"\n{'='*80}")
print("ğŸ”§ BASELINE FORMAT (tá»« dataloader.py):")
print(f"{'='*80}")
baseline_format = f"Question: {sample['question']}\nAnswer: {sample['answer']}"
print(baseline_format)

# Tokenize Ä‘á»ƒ xem length
print(f"\n{'='*80}")
print("ğŸ“ TOKEN LENGTH CHECK:")
print(f"{'='*80}")
tokens = tokenizer(baseline_format, return_tensors='pt')
print(f"   Token length: {len(tokens['input_ids'][0])}")
print(f"   First 50 tokens:")
print(f"   {tokenizer.decode(tokens['input_ids'][0][:50])}")

# So sÃ¡nh vá»›i HDP format
print(f"\n{'='*80}")
print("ğŸ†š SO SÃNH Vá»šI HDP FORMAT:")
print(f"{'='*80}")

# Load hierarchical data
import json
with open('/workspace/hdp-diffusion/data/gsm8k/gsm8k_hierarchical_train.json', 'r') as f:
    hdp_data = json.load(f)

hdp_sample = hdp_data[0]
print(f"\nğŸ“ HDP Sample:")
print(f"   Question: {hdp_sample['question'][:100]}...")
print(f"   Plan: {hdp_sample['plan'][:100]}...")
print(f"   Execution: {hdp_sample['execution'][:100]}...")
print(f"   Answer: {hdp_sample['answer']}")
print(f"\n   Full text (model output):")
print(f"   {hdp_sample['full_text'][:200]}...")

# Token counts
baseline_tokens = len(tokenizer(baseline_format)['input_ids'])
hdp_question = len(tokenizer(hdp_sample['question'])['input_ids'])
hdp_plan = len(tokenizer(hdp_sample['plan'])['input_ids'])
hdp_exec = len(tokenizer(hdp_sample['execution'])['input_ids'])

print(f"\n{'='*80}")
print("ğŸ“Š TOKEN COUNT COMPARISON:")
print(f"{'='*80}")
print(f"   Baseline total: ~{baseline_tokens} tokens")
print(f"   HDP Question: ~{hdp_question} tokens")
print(f"   HDP Plan: ~{hdp_plan} tokens")
print(f"   HDP Execution: ~{hdp_exec} tokens")
print(f"   HDP Total: ~{hdp_question + hdp_plan + hdp_exec} tokens")

print(f"\n{'='*80}")
print("âœ… Káº¾T LUáº¬N:")
print(f"{'='*80}")
print("""
BASELINE FORMAT:
  - Simple concatenation: "Question: ... \\nAnswer: ..."
  - KhÃ´ng cÃ³ phÃ¢n chia structure
  - Token length biáº¿n Ä‘á»•i tÃ¹y sample
  
HDP FORMAT:
  - 3 blocks rÃµ rÃ ng: [Question | Plan | Execution]
  - CÃ³ special tokens: [PLAN], [EXECUTION], [ANSWER]
  - Fixed length má»—i block: 128 + 128 + 256 = 512 tokens
  - Hierarchical attention: Plan khÃ´ng nhÃ¬n tháº¥y Execution
""")

# ğŸ¯ GSM8K HIERARCHICAL TRAINING - COMPLETE SETUP

## TÃ³m táº¯t nhanh

ÄÃ£ setup Ä‘áº§y Ä‘á»§ pipeline Ä‘á»ƒ train Hierarchical BD3-LM trÃªn GSM8K vá»›i má»¥c Ä‘Ã­ch chá»©ng minh:
- **Plan Module** há»c abstract reasoning (khÃ´ng cÃ³ sá»‘ cá»¥ thá»ƒ)
- **Execution Module** há»c concrete calculations (vá»›i sá»‘)

---

## ğŸ“¦ Files Ä‘Ã£ táº¡o cho GSM8K

### 1. Data Preparation (2 files)
| File | MÃ´ táº£ |
|------|-------|
| `scripts/data_prep/generate_gsm8k_plans.py` | Script Python Ä‘á»ƒ generate plans vá»›i vLLM + Llama-3 |
| `scripts/data_prep/run_gsm8k_plan_generation.sh` | SLURM script Ä‘á»ƒ cháº¡y trÃªn H200 |

**Chá»©c nÄƒng:**
- Load GSM8K tá»« HuggingFace (7.5k train, 1.3k test)
- DÃ¹ng Llama-3-8B-Instruct Ä‘á»ƒ extract high-level plan
- Save thÃ nh format: `[Question, Plan, Execution]`

### 2. Data Loading (1 file)
| File | MÃ´ táº£ |
|------|-------|
| `gsm8k_dataloader.py` | PyTorch Dataset + analysis tools |

**Chá»©c nÄƒng:**
- `GSM8KHierarchicalDataset`: Load JSON vÃ o PyTorch format
- `analyze_gsm8k_lengths()`: Analyze token lengths
- Integration vá»›i `HierarchicalDataCollator`

### 3. Training (1 file)
| File | MÃ´ táº£ |
|------|-------|
| `scripts/train/train_gsm8k_hierarchical.sh` | Training script cho GSM8K |

**Configuration:**
- Question: 128 tokens
- Plan: 128 tokens
- Execution: 256 tokens
- Total: 512 tokens
- Block size: 16

### 4. Documentation (2 files)
| File | MÃ´ táº£ |
|------|-------|
| `GSM8K_TRAINING_GUIDE.md` | Full guide (step-by-step) |
| `test_gsm8k_pipeline.sh` | Test script Ä‘á»ƒ verify setup |

---

## ğŸš€ Quick Start (3 bÆ°á»›c)

### BÆ°á»›c 1: Generate Plans

```bash
# Cháº¡y trÃªn H200 vá»›i vLLM
sbatch scripts/data_prep/run_gsm8k_plan_generation.sh

# Hoáº·c test nhanh vá»›i 100 examples:
python scripts/data_prep/generate_gsm8k_plans.py \
    --model meta-llama/Meta-Llama-3-8B-Instruct \
    --split train \
    --batch_size 256 \
    --num_examples 100 \
    --output_path data/gsm8k/test_100.json
```

**Thá»i gian:**
- 100 examples: ~2 phÃºt
- 7,500 examples: ~30-40 phÃºt (H200)

**Output:** `data/gsm8k/gsm8k_hierarchical_train.json`

### BÆ°á»›c 2: Verify Data

```bash
# Check file
ls -lh data/gsm8k/

# Analyze token lengths
python gsm8k_dataloader.py \
    --data_path data/gsm8k/gsm8k_hierarchical_train.json \
    --analyze

# View first example
jq '.[0]' data/gsm8k/gsm8k_hierarchical_train.json
```

### BÆ°á»›c 3: Train Model

```bash
# Full training
sbatch scripts/train/train_gsm8k_hierarchical.sh

# Or quick test (1000 steps)
python main.py \
    mode=train \
    model=tiny \
    training.max_steps=1000 \
    training.hierarchical.enabled=true \
    data.train_path=data/gsm8k/gsm8k_hierarchical_train.json
```

---

## ğŸ“Š Expected Data Format

### Input (after plan generation):

```json
{
  "id": "gsm8k_train_0",
  "question": "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?",
  "plan": "Calculate daily egg production. Determine total consumption from breakfast and baking. Subtract consumption from production to find remainder. Multiply remainder by unit price to get daily revenue.",
  "execution": "Janet gets 16 eggs per day. She eats 3 for breakfast and uses 4 for muffins, totaling 7 eggs. This leaves 16 - 7 = 9 eggs to sell. She sells them for $2 each, making 9 * 2 = $18 per day.",
  "answer_numerical": "18"
}
```

### Key observations:

âœ… **Question:** Original problem (65 tokens avg)
âœ… **Plan:** Abstract reasoning, NO specific numbers (42 tokens avg)
âœ… **Execution:** Concrete calculations with numbers (98 tokens avg)
âœ… **Answer:** Numerical result for evaluation

---

## ğŸ¨ Plan Generation Prompt

The key to good plans is the prompt:

```python
PLAN_EXTRACTION_PROMPT = """
You are a helpful assistant. Given a math problem and its solution, 
extract the high-level plan or reasoning skeleton.

IMPORTANT RULES:
1. The plan should contain ONLY logical steps and operations
2. Do NOT include specific numbers or calculations
3. Use abstract terms like "initial quantity", "given amount"
4. Keep it concise (2-4 sentences)
5. Focus on reasoning structure, not execution

Example:
Question: "Lan cÃ³ 5 quáº£ tÃ¡o, cho Ä‘i 2. Há»i cÃ²n máº¥y?"
Solution: "Lan cho Ä‘i nghÄ©a lÃ  phÃ©p trá»«. 5 - 2 = 3. Lan cÃ²n 3 quáº£."
Plan: "Identify the initial quantity. Use subtraction operation for 
       the given amount. Conclude the remaining quantity."
"""
```

**Náº¿u plans cÃ³ quÃ¡ nhiá»u sá»‘:**
- TÄƒng emphasis trong prompt
- Add few-shot examples
- Increase temperature (0.8-0.9)

---

## ğŸ”¬ Experiments cho Paper

### 1. Plan Abstraction Analysis

**Hypothesis:** Plan module há»c abstract reasoning (khÃ´ng cÃ³ sá»‘)

**Metrics:**
```python
import re

# Count numbers in plans vs executions
plan_numbers = [len(re.findall(r'\d+', x['plan'])) for x in data]
exec_numbers = [len(re.findall(r'\d+', x['execution'])) for x in data]

print(f"Avg numbers in Plan: {np.mean(plan_numbers):.2f}")
print(f"Avg numbers in Exec: {np.mean(exec_numbers):.2f}")

# Expected: Plan << Execution
```

### 2. Hierarchical vs Flat Comparison

**Setup:**
- Model A: Hierarchical (Question â†’ Plan â†’ Execution)
- Model B: Flat (Question â†’ Execution directly)
- Model C: Baseline AR

**Metrics:**
- Perplexity on test set
- Accuracy (% correct answers)
- Inference speed

### 3. Transfer Learning

**Setup:**
1. Train on GSM8K
2. Freeze Plan module
3. Fine-tune on SVAMP/MultiArith
4. Compare with baseline

**Hypothesis:** Abstract plans transfer better

---

## ğŸ“ˆ Expected Results

### Token Length Statistics (95th percentile):
- Question: ~105 tokens â†’ use 128
- Plan: ~75 tokens â†’ use 128
- Execution: ~165 tokens â†’ use 256
- **Total: 512 tokens**

### Training Metrics:
- Initial loss: ~8-9
- After 10K steps: ~5-6
- After 50K steps: ~4-5
- Convergence: ~50K steps (on 7.5k examples)

### Generation Quality:
- Plan abstraction: <5% should contain numbers
- Execution detail: >80% should have calculations
- Answer accuracy: 60-70% (if evaluated)

---

## âœ… Verification Checklist

### Data Preparation
- [ ] Install vLLM: `pip install vllm`
- [ ] Generate plans: `sbatch scripts/data_prep/run_gsm8k_plan_generation.sh`
- [ ] Check output: `jq length data/gsm8k/gsm8k_hierarchical_train.json` â†’ 7473
- [ ] Analyze lengths: `python gsm8k_dataloader.py --analyze`
- [ ] Manual inspection: Check 10-20 examples for plan quality

### Training Setup
- [ ] Test mask: `python test_hierarchical_mask.py`
- [ ] Test dataloader: `python gsm8k_dataloader.py --data_path ...`
- [ ] Test pipeline: `bash test_gsm8k_pipeline.sh`
- [ ] Small training run (1K steps): Verify loss decreases

### Full Training
- [ ] Start training: `sbatch scripts/train/train_gsm8k_hierarchical.sh`
- [ ] Monitor logs: `tail -f logs/gsm8k_hier_bd3lm_*.out`
- [ ] Check checkpoints: `ls outputs/gsm8k_hierarchical_bd3lm_bs16/`
- [ ] Training time: ~24 hours for 50K steps (4 GPUs)

### Evaluation
- [ ] Generate samples from checkpoint
- [ ] Calculate perplexity on test set
- [ ] Analyze plan abstraction
- [ ] Compare with baselines (AR, MDLM)
- [ ] Measure accuracy (if applicable)

---

## ğŸ› Common Issues

### Issue 1: vLLM installation fails
```bash
# Try specific version
pip install vllm==0.4.0

# Or use conda
conda install -c conda-forge vllm
```

### Issue 2: Plans contain numbers
**Solution:** Improve prompt (see section above)

### Issue 3: CUDA OOM during training
```bash
# Reduce batch size
BATCH_SIZE=16

# Use smaller model
MODEL_SIZE=tiny

# Reduce sequence length
TOTAL_LEN=384  # instead of 512
```

### Issue 4: Training loss not decreasing
1. Verify mask: `python test_hierarchical_mask.py`
2. Check data format: Print 5 samples
3. Reduce LR: `LR=1e-4`
4. Increase warmup: `WARMUP_STEPS=10000`

---

## ğŸ“š File Structure

```
hdp-diffusion/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_prep/
â”‚   â”‚   â”œâ”€â”€ generate_gsm8k_plans.py        # âœ¨ NEW: Plan generation
â”‚   â”‚   â””â”€â”€ run_gsm8k_plan_generation.sh   # âœ¨ NEW: SLURM script
â”‚   â””â”€â”€ train/
â”‚       â””â”€â”€ train_gsm8k_hierarchical.sh    # âœ¨ NEW: Training script
â”‚
â”œâ”€â”€ gsm8k_dataloader.py                    # âœ¨ NEW: GSM8K dataset loader
â”œâ”€â”€ GSM8K_TRAINING_GUIDE.md                # âœ¨ NEW: Full guide
â”œâ”€â”€ test_gsm8k_pipeline.sh                 # âœ¨ NEW: Test script
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ gsm8k/
â”‚       â”œâ”€â”€ gsm8k_hierarchical_train.json  # Generated plans (train)
â”‚       â””â”€â”€ gsm8k_hierarchical_test.json   # Generated plans (test)
â”‚
â””â”€â”€ outputs/
    â””â”€â”€ gsm8k_hierarchical_bd3lm_bs16/     # Training checkpoints
```

---

## ğŸ¯ Timeline

### Day 1: Data Preparation
- [ ] Run plan generation (~1 hour with H200)
- [ ] Verify data quality
- [ ] Analyze token lengths

### Day 2-3: Training
- [ ] Start training (50K steps)
- [ ] Monitor progress
- [ ] Save checkpoints

### Day 4-5: Evaluation
- [ ] Generate samples
- [ ] Calculate metrics
- [ ] Analyze plan quality

### Week 2: Analysis
- [ ] Compare with baselines
- [ ] Run ablation studies
- [ ] Prepare results for paper

---

## ğŸ’¡ Tips for Paper

### Claims to make:

1. **Hierarchical structure enables abstraction:**
   - Show plan has <5% numbers
   - Show execution has >80% calculations
   - Visualize attention patterns

2. **Better transfer learning:**
   - Train on GSM8K
   - Test on SVAMP/MultiArith
   - Compare with flat baseline

3. **Interpretability:**
   - Plans are human-readable
   - Can edit plans for different solutions
   - Modular reasoning (H-Module + L-Module)

### Figures to include:

1. **Figure 1:** Attention mask visualization
2. **Figure 2:** Examples of Question/Plan/Execution
3. **Figure 3:** Plan abstraction analysis (histogram of numbers)
4. **Figure 4:** Transfer learning results
5. **Table 1:** Comparison with baselines (perplexity, accuracy)

---

## ğŸš€ Summary

**âœ… Setup hoÃ n táº¥t:**
- Pipeline Ä‘á»ƒ generate plans tá»« GSM8K
- Dataloader cho hierarchical format
- Training script adapted cho math reasoning
- Full documentation & testing tools

**ğŸ“¦ Deliverables:**
- 6 new files (scripts, dataloader, docs)
- Integration vá»›i existing hierarchical system
- Ready to run on H200

**ğŸ¯ Next:**
1. Generate plans: `sbatch scripts/data_prep/run_gsm8k_plan_generation.sh`
2. Start training: `sbatch scripts/train/train_gsm8k_hierarchical.sh`
3. Evaluate & analyze results

**Start here:** [GSM8K_TRAINING_GUIDE.md](GSM8K_TRAINING_GUIDE.md) ğŸš€

# TÃ“M Táº®T THá»°C HIá»†N - HIERARCHICAL DISCRETE DIFFUSION

## âœ… CÃC BÆ¯á»šC ÄÃƒ HOÃ€N THÃ€NH

### BÆ°á»›c 1: âœ… TÃ¬m vÃ  hiá»ƒu cáº¥u trÃºc Mask

**Vá»‹ trÃ­:** `models/dit.py` (dÃ²ng 30-75)

**PhÃ¡t hiá»‡n:**
- Function `block_diff_mask()` Ä‘á»‹nh nghÄ©a mask gá»‘c cho BD3-LM
- Function `gen_mask()` (dÃ²ng 706) táº¡o mask cho attention
- Mask Ä‘Æ°á»£c lÆ°u trong `self.block_diff_mask`
- Há»— trá»£ 2 backend: `sdpa` vÃ  `flex` (FlexAttention)

### BÆ°á»›c 2: âœ… Táº¡o Hierarchical Mask

**File má»›i:** `models/hierarchical_mask.py`

**Chá»©c nÄƒng:**
```python
hierarchical_block_diff_mask(
    b, h, q_idx, kv_idx,
    question_len, plan_len, exec_len, 
    block_size, n
)
```

**Cáº¥u trÃºc Mask (theo yÃªu cáº§u):**
- âœ… Question â†’ Question: Full attention
- âœ… Plan â†’ Question: Full attention  
- âœ… Plan â†’ Plan: Block diffusion pattern
- âŒ Plan â†’ Execution: **BLOCKED** (giá»¯ tÃ­nh nhÃ¢n quáº£)
- âœ… Execution â†’ Question: Full attention
- âœ… Execution â†’ Plan: Full attention
- âœ… Execution â†’ Execution: Block diffusion pattern

**Format sequence:**
```
[Question | Plan_xt | Plan_x0 | Exec_xt | Exec_x0]
   256    |   256    |   256   |   512   |   512   = 1792 tokens total
```

### BÆ°á»›c 3: âœ… Xá»­ lÃ½ Input Data

**File má»›i:** `hierarchical_dataloader.py`

**Class chÃ­nh:**

1. **HierarchicalDataCollator**
   - Nháº­n input: `{'question': ..., 'plan': ..., 'execution': ...}`
   - Hoáº·c: `{'text': ...}` (tá»± Ä‘á»™ng split)
   - Output: Tensor shape `[batch, question_len + plan_len + exec_len]`

2. **create_hierarchical_dataset()**
   - Táº¡o dataset tá»« OpenWebText hoáº·c data khÃ¡c
   - Tá»± Ä‘á»™ng split thÃ nh 3 pháº§n
   
3. **load_reasoning_dataset()**
   - Load tá»« JSON Ä‘Ã£ format sáºµn
   - Format: `[{"question": ..., "plan": ..., "execution": ...}, ...]`

**CÃ¡ch sá»­ dá»¥ng:**
```python
from hierarchical_dataloader import HierarchicalDataCollator

collator = HierarchicalDataCollator(
    tokenizer=tokenizer,
    question_len=256,
    plan_len=256, 
    exec_len=512
)

batch = collator(examples)
# Output: {'input_ids': tensor, 'attention_mask': tensor, 'hierarchical_info': dict}
```

### BÆ°á»›c 4: âœ… TÃ­ch há»£p vÃ o Model

**Thay Ä‘á»•i trong:** `models/dit.py`

**Function má»›i:**
```python
def gen_mask(self, seqlen, block_size, attn_backend='sdpa', hierarchical_config=None):
    if hierarchical_config is not None:
        # Use hierarchical mask
        from models.hierarchical_mask import create_hierarchical_mask
        self.block_diff_mask = create_hierarchical_mask(...)
    else:
        # Use original BD3-LM mask
        ...
```

### BÆ°á»›c 5: âœ… Táº¯t/ÄÆ¡n giáº£n hÃ³a tÃ­nh nÄƒng khÃ´ng cáº§n thiáº¿t

**Config:** `configs/algo/hierarchical.yaml`

```yaml
hierarchical:
  enabled: true
  question_len: 256
  plan_len: 256
  exec_len: 512

training:
  use_hierarchical_collator: true
  var_length_gen: false  # âŒ Táº¯t arbitrary-length

sampling:
  hierarchical_mode: 'full'
  var_length: false      # âŒ Táº¯t variable-length
  first_hitting: true    # âœ… Báº­t (nhanh hÆ¡n)
  kv_cache: true         # âœ… Báº­t (tÄƒng tá»‘c)
```

### BÆ°á»›c 6: âœ… Training Script

**File má»›i:** `scripts/train/train_hierarchical_bd3lm.sh`

```bash
#!/bin/bash
QUESTION_LEN=256
PLAN_LEN=256
EXEC_LEN=512
BLOCK_SIZE=16

python -u main.py \
    mode=train \
    model=small \
    model.length=1024 \
    algo=bd3lm \
    block_size=${BLOCK_SIZE} \
    training.hierarchical.enabled=true \
    training.hierarchical.question_len=${QUESTION_LEN} \
    training.hierarchical.plan_len=${PLAN_LEN} \
    training.hierarchical.exec_len=${EXEC_LEN}
```

### BÆ°á»›c 7: âœ… Testing & Documentation

**Files:**
- `test_hierarchical_mask.py`: Test script Ä‘á»ƒ verify mask
- `HIERARCHICAL_README.md`: Documentation Ä‘áº§y Ä‘á»§ báº±ng tiáº¿ng Viá»‡t

## ğŸ“‹ CÃCH Sá»¬ Dá»¤NG NHANH

### 1. Chuáº©n bá»‹ data

**Option A: Dá»¯ liá»‡u cÃ³ sáºµn cáº¥u trÃºc**
```json
// data.json
[
  {
    "question": "What is the capital of France?",
    "plan": "I need to recall European geography and capitals.",
    "execution": "Paris is the capital and largest city of France, located in the north-central part of the country."
  }
]
```

**Option B: Tá»± Ä‘á»™ng split tá»« vÄƒn báº£n**
```python
# Repo sáº½ tá»± Ä‘á»™ng chia:
# - 25% Ä‘áº§u â†’ Question
# - 25% tiáº¿p â†’ Plan  
# - 50% cÃ²n láº¡i â†’ Execution
```

### 2. Training

```bash
# Cháº¡y training script
sbatch scripts/train/train_hierarchical_bd3lm.sh

# Hoáº·c run trá»±c tiáº¿p
python main.py \
    mode=train \
    model=small \
    algo=bd3lm \
    block_size=16 \
    training.hierarchical.enabled=true \
    training.hierarchical.question_len=256 \
    training.hierarchical.plan_len=256 \
    training.hierarchical.exec_len=512
```

### 3. Test mask (trÆ°á»›c khi train)

```bash
python test_hierarchical_mask.py
# Output: 
#  - âœ… Verification results
#  - ğŸ“Š Visualization: hierarchical_mask_test.png
```

### 4. Customize cho domain cá»§a báº¡n

**Sá»­a logic split trong `hierarchical_dataloader.py`:**

```python
def process_example(example):
    text = example['text']
    
    # TODO: Replace with your logic
    # VÃ­ dá»¥:
    # - Parse tá»« markdown structure
    # - DÃ¹ng regex tÃ¡ch sections
    # - DÃ¹ng model khÃ¡c Ä‘á»ƒ identify
    
    question = your_extract_question_logic(text)
    plan = your_extract_plan_logic(text)
    execution = your_extract_execution_logic(text)
    
    return {
        'question': tokenizer.encode(question),
        'plan': tokenizer.encode(plan),
        'execution': tokenizer.encode(execution),
    }
```

## ğŸ“ Cáº¤U TRÃšC FILES Má»šI

```
hdp-diffusion/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hierarchical_mask.py          # âœ¨ NEW: Hierarchical attention mask
â”‚   â””â”€â”€ dit.py                         # ğŸ”§ MODIFIED: Added hierarchical support
â”‚
â”œâ”€â”€ hierarchical_dataloader.py         # âœ¨ NEW: Data collator & dataset utils
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ algo/
â”‚       â””â”€â”€ hierarchical.yaml          # âœ¨ NEW: Hierarchical config
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train/
â”‚       â””â”€â”€ train_hierarchical_bd3lm.sh # âœ¨ NEW: Training script
â”‚
â”œâ”€â”€ test_hierarchical_mask.py          # âœ¨ NEW: Test & verification
â”‚
â”œâ”€â”€ HIERARCHICAL_README.md             # âœ¨ NEW: Full documentation (Vietnamese)
â”‚
â””â”€â”€ IMPLEMENTATION_SUMMARY.md          # âœ¨ NEW: This file
```

## ğŸ” ÄIá»‚M QUAN TRá»ŒNG Cáº¦N LÆ¯U Ã

### 1. Attention Mask Structure

Mask pháº£i Ä‘áº£m báº£o:
```
âœ… Plan cÃ³ thá»ƒ "Ä‘á»c" Question (Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh)
âœ… Execution cÃ³ thá»ƒ "Ä‘á»c" cáº£ Question vÃ  Plan
âŒ Plan KHÃ”NG thá»ƒ "Ä‘á»c" Execution (tÃ­nh nhÃ¢n quáº£)
```

Kiá»ƒm tra báº±ng test:
```python
# Plan cannot see Execution
plan_to_exec = mask[question_end:plan_x0_end, plan_x0_end:]
assert not plan_to_exec.any(), "Plan should NOT see Execution!"
```

### 2. Data Format

**Sequence structure:**
```
Input:  [Q Q Q ... | P P P ... | E E E E ...]
        â””â”€ 256 â”€â”€â”€â”˜ â””â”€ 256 â”€â”€â”˜ â””â”€â”€ 512 â”€â”€â”€â”˜

For training (with xt and x0):
[Q | Plan_xt | Plan_x0 | Exec_xt | Exec_x0]
 â””â”€ 256 â”€â”˜ â””â”€â”€ 256 â”€â”€â”˜ â””â”€â”€ 256 â”€â”€â”˜ â””â”€â”€ 512 â”€â”€â”˜ â””â”€â”€ 512 â”€â”€â”˜
```

### 3. Block Size Trade-off

| Block Size | Speed | Quality | Use Case |
|-----------|-------|---------|----------|
| 1 | Cháº­m nháº¥t | Tá»‘t nháº¥t | Baseline (AR) |
| 4-8 | Trung bÃ¬nh | Ráº¥t tá»‘t | Research |
| 16-32 | Nhanh | Tá»‘t | Production |
| 1024 | Nhanh nháº¥t | Tháº¥p hÆ¡n | Fast inference |

**Khuyáº¿n nghá»‹:** Báº¯t Ä‘áº§u vá»›i `block_size=16`

### 4. Integration vá»›i Codebase Gá»‘c

**KhÃ´ng cáº§n sá»­a nhiá»u!** Chá»‰ cáº§n:

```python
# In diffusion.py or main training loop:
if config.training.hierarchical.enabled:
    # Use hierarchical mask
    model.backbone.gen_mask(
        seqlen=config.model.length,
        block_size=config.block_size,
        hierarchical_config={
            'question_len': config.training.hierarchical.question_len,
            'plan_len': config.training.hierarchical.plan_len,
            'exec_len': config.training.hierarchical.exec_len,
        }
    )
    
    # Use hierarchical collator
    from hierarchical_dataloader import HierarchicalDataCollator
    collator = HierarchicalDataCollator(tokenizer, ...)
else:
    # Original BD3-LM behavior
    model.backbone.gen_mask(seqlen, block_size)
```

## âš ï¸ TROUBLESHOOTING

### Lá»—i thÆ°á»ng gáº·p:

1. **"Mask dimensions don't match"**
   - Kiá»ƒm tra: `total_len = question_len + plan_len*2 + exec_len*2`
   - Äáº£m báº£o nhÃ¢n 2 vÃ¬ cÃ³ cáº£ xt vÃ  x0

2. **"CUDA out of memory"**
   - Giáº£m batch size: `loader.batch_size=32`
   - Giáº£m model size: `model=tiny`
   - Giáº£m sequence length

3. **Model khÃ´ng há»c Ä‘Æ°á»£c**
   - Test mask trÆ°á»›c: `python test_hierarchical_mask.py`
   - Visualize attention patterns
   - Check data format: In ra vÃ i samples
   - Giáº£m learning rate: `optim.lr=1e-4`

4. **"FlexAttention not available"**
   - DÃ¹ng SDPA thay vÃ¬: `model.attn_backend=sdpa`
   - Hoáº·c cÃ i: `pip install flash-attn==2.5.6`

## ğŸš€ NEXT STEPS

### Äá»ƒ cháº¡y thá»­ ngay:

1. **Test mask (khÃ´ng cáº§n GPU):**
   ```bash
   python test_hierarchical_mask.py
   ```

2. **Prepare data nhá» Ä‘á»ƒ test:**
   ```python
   from hierarchical_dataloader import load_reasoning_dataset
   
   # Create small test dataset (100 examples)
   dataset = create_test_data(num_examples=100)
   ```

3. **Run training trÃªn data nhá»:**
   ```bash
   python main.py \
       mode=train \
       model=tiny \
       training.max_steps=1000 \
       training.hierarchical.enabled=true
   ```

4. **Verify loss giáº£m:**
   - Check tensorboard/wandb logs
   - Loss nÃªn giáº£m sau ~100 steps

5. **Scale lÃªn:**
   - TÄƒng data size
   - TÄƒng model size: `model=small`
   - TÄƒng training steps: `training.max_steps=100000`

### Äá»ƒ customize cho domain cá»§a báº¡n:

1. **Implement data parser:**
   - Sá»­a `hierarchical_dataloader.py`
   - Function `process_example()`
   - Parse theo format cá»§a báº¡n

2. **Tune hyperparameters:**
   - Äá»™ dÃ i: `question_len`, `plan_len`, `exec_len`
   - Block size: `4, 8, 16, 32`
   - Learning rate, warmup, etc.

3. **Add evaluation metrics:**
   - Äo cháº¥t lÆ°á»£ng Plan riÃªng
   - Äo cháº¥t lÆ°á»£ng Execution riÃªng
   - Äo coherence giá»¯a Plan vÃ  Execution

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- **Paper gá»‘c:** Block Diffusion (ICLR 2025)
- **Appendix B.6, B.7:** Hierarchical architecture design
- **Figure 4:** Attention mask visualization
- **HIERARCHICAL_README.md:** Full documentation

## âœ… CHECKLIST TRÆ¯á»šC KHI TRAIN

- [ ] Test mask: `python test_hierarchical_mask.py`
- [ ] Check visualization: `hierarchical_mask_test.png`
- [ ] Prepare data (100-1000 examples Ä‘á»ƒ test)
- [ ] Verify data format: Print ra vÃ i samples
- [ ] Set hyperparameters trong config
- [ ] Táº¡o output directory: `mkdir -p outputs/hierarchical_test`
- [ ] Run small test: 1000 steps, tiny model
- [ ] Check loss giáº£m
- [ ] Scale lÃªn full training

---

**TÃ³m láº¡i:** ÄÃ£ implement Ä‘áº§y Ä‘á»§ 3 bÆ°á»›c theo yÃªu cáº§u cá»§a báº¡n:
1. âœ… TÃ¬m vÃ  modify mask (Plan khÃ´ng tháº¥y Execution)
2. âœ… Xá»­ lÃ½ input data thÃ nh [Question, Plan, Execution]
3. âœ… Táº¯t arbitrary-length generation, giá»¯ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh

Code Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ test vÃ  cháº¡y thá»­! ğŸ‰

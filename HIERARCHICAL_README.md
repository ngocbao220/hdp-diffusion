# Hierarchical Discrete Diffusion: Plan-then-Generate Architecture

## Tá»•ng quan (Overview)

Repo nÃ y Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ há»— trá»£ **mÃ´ hÃ¬nh suy luáº­n phÃ¢n táº§ng** (hierarchical reasoning) dá»±a trÃªn cÆ¡ cháº¿ khá»‘i vÃ  máº·t náº¡ chÃº Ã½ (block diffusion with hierarchical attention masks).

### Kiáº¿n trÃºc chÃ­nh:
```
[Question] â†’ [Plan Block] â†’ [Execution Block]
    â†“            â†“                  â†“
  Context    High-level      Detailed steps
             reasoning
```

### NguyÃªn táº¯c Attention Mask:
- âœ… **Plan Block** cÃ³ thá»ƒ nhÃ¬n tháº¥y **Question**
- âœ… **Execution Block** cÃ³ thá»ƒ nhÃ¬n tháº¥y **Question** vÃ  **Plan Block**  
- âŒ **Plan Block** KHÃ”NG thá»ƒ nhÃ¬n tháº¥y **Execution Block** (giá»¯ tÃ­nh nhÃ¢n quáº£)

## CÃ¡c file má»›i Ä‘Æ°á»£c thÃªm vÃ o

### 1. `models/hierarchical_mask.py`
Äá»‹nh nghÄ©a attention mask phÃ¢n táº§ng theo Figure 4 trong paper (Appendix B.6, B.7).

**Chá»©c nÄƒng chÃ­nh:**
- `hierarchical_block_diff_mask()`: Táº¡o mask cho cáº¥u trÃºc Plan-then-Generate
- `create_hierarchical_mask()`: Helper function Ä‘á»ƒ khá»Ÿi táº¡o mask

**Cáº¥u trÃºc mask:**
```
Sequence: [Question | Plan_xt | Plan_x0 | Exec_xt | Exec_x0]

Question tokens: Full self-attention
Plan tokens:     Can see Question + Plan (block diffusion pattern)
Exec tokens:     Can see Question + Plan + Exec (block diffusion pattern)
```

### 2. `hierarchical_dataloader.py`
Xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº§u vÃ o cho training vÃ  inference.

**CÃ¡c class chÃ­nh:**
- `HierarchicalDataCollator`: Collate data thÃ nh format [Question, Plan, Execution]
- `create_hierarchical_dataset()`: Táº¡o dataset tá»« dá»¯ liá»‡u thÃ´
- `load_reasoning_dataset()`: Load dataset Ä‘Ã£ Ä‘Æ°á»£c format sáºµn

**Input format:**
```json
{
  "question": "What is the capital of France?",
  "plan": "I need to recall European capitals...",
  "execution": "Paris is the capital and largest city..."
}
```

### 3. `configs/algo/hierarchical.yaml`
Config cho hierarchical training.

**Tham sá»‘ chÃ­nh:**
```yaml
hierarchical:
  question_len: 256  # Äá»™ dÃ i pháº§n question
  plan_len: 256      # Äá»™ dÃ i Plan Block
  exec_len: 512      # Äá»™ dÃ i Execution Block
```

### 4. `scripts/train/train_hierarchical_bd3lm.sh`
Script Ä‘á»ƒ cháº¡y training cho mÃ´ hÃ¬nh hierarchical.

## HÆ°á»›ng dáº«n sá»­ dá»¥ng (Quick Start)

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u

CÃ³ 2 cÃ¡ch chuáº©n bá»‹ dá»¯ liá»‡u:

#### Option A: Dá»¯ liá»‡u Ä‘Ã£ cÃ³ cáº¥u trÃºc [Question, Plan, Execution]

Táº¡o file JSON:
```json
[
  {
    "question": "Your question here...",
    "plan": "High-level plan...",
    "execution": "Detailed execution..."
  },
  ...
]
```

Sau Ä‘Ã³ load trong code:
```python
from hierarchical_dataloader import load_reasoning_dataset

dataset = load_reasoning_dataset(
    dataset_path='path/to/your/data.json',
    tokenizer=tokenizer,
    question_len=256,
    plan_len=256,
    exec_len=512
)
```

#### Option B: Tá»± Ä‘á»™ng split tá»« vÄƒn báº£n dÃ i

Náº¿u báº¡n cÃ³ dá»¯ liá»‡u dáº¡ng vÄƒn báº£n thÃ´ng thÆ°á»ng (nhÆ° OpenWebText), collator sáº½ tá»± Ä‘á»™ng chia:
- 25% Ä‘áº§u â†’ Question
- 25% tiáº¿p theo â†’ Plan  
- 50% cÃ²n láº¡i â†’ Execution

**LÆ°u Ã½:** CÃ¡ch nÃ y chá»‰ lÃ  placeholder. Báº¡n nÃªn implement logic split phÃ¹ há»£p vá»›i domain cá»§a mÃ¬nh.

### BÆ°á»›c 2: Training

```bash
# Sá»­a tham sá»‘ trong script náº¿u cáº§n
vim scripts/train/train_hierarchical_bd3lm.sh

# Cháº¡y training
sbatch scripts/train/train_hierarchical_bd3lm.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:
```bash
python main.py \
    mode=train \
    model=small \
    model.length=1024 \
    algo=bd3lm \
    block_size=16 \
    training.hierarchical.enabled=true \
    training.hierarchical.question_len=256 \
    training.hierarchical.plan_len=256 \
    training.hierarchical.exec_len=512
```

### BÆ°á»›c 3: Inference

```python
from models.hierarchical_mask import create_hierarchical_mask
from hierarchical_dataloader import HierarchicalDataCollator

# Initialize model with hierarchical mask
model.gen_mask(
    seqlen=1024,
    block_size=16,
    hierarchical_config={
        'question_len': 256,
        'plan_len': 256,
        'exec_len': 512
    }
)

# Prepare input
collator = HierarchicalDataCollator(tokenizer, 256, 256, 512)
batch = collator([{'question': question_tokens, ...}])

# Generate
samples = model.restore_model_and_sample(num_steps=50)
```

## Kiá»ƒm tra Attention Mask

Äá»ƒ visualize mask vÃ  Ä‘áº£m báº£o nÃ³ Ä‘Ãºng nhÆ° mong muá»‘n:

```python
from models.hierarchical_mask import create_hierarchical_mask
import matplotlib.pyplot as plt

mask = create_hierarchical_mask(
    seqlen=1024,
    block_size=16,
    question_len=256,
    plan_len=256,
    exec_len=512,
    attn_backend='sdpa'
)

# Visualize
plt.figure(figsize=(12, 12))
plt.imshow(mask.float(), cmap='binary')
plt.title('Hierarchical Attention Mask')
plt.xlabel('Key Position')
plt.ylabel('Query Position')
plt.colorbar(label='Can Attend')

# Add boundary lines
plt.axvline(x=256, color='r', linestyle='--', label='Question/Plan boundary')
plt.axvline(x=512, color='g', linestyle='--', label='Plan/Exec boundary')
plt.axhline(y=256, color='r', linestyle='--')
plt.axhline(y=512, color='g', linestyle='--')
plt.legend()
plt.savefig('hierarchical_mask_visualization.png')
```

## TÃ¹y chá»‰nh cho domain cá»§a báº¡n

### 1. Sá»­a logic splitting trong DataCollator

File: `hierarchical_dataloader.py`

TÃ¬m function `process_example()` vÃ  implement logic riÃªng:

```python
def process_example(example):
    text = example['text']
    
    # TODO: Implement your domain-specific logic
    # VÃ­ dá»¥: 
    # - DÃ¹ng regex Ä‘á»ƒ tÃ¡ch sections
    # - DÃ¹ng model khÃ¡c Ä‘á»ƒ identify plan vs execution
    # - Parse tá»« structured format
    
    question = extract_question(text)
    plan = extract_plan(text)
    execution = extract_execution(text)
    
    return {
        'question': tokenizer.encode(question),
        'plan': tokenizer.encode(plan),
        'execution': tokenizer.encode(execution),
    }
```

### 2. Äiá»u chá»‰nh Ä‘á»™ dÃ i blocks

TÃ¹y vÃ o task, báº¡n cÃ³ thá»ƒ cáº§n thay Ä‘á»•i:

```yaml
# configs/algo/hierarchical.yaml
hierarchical:
  question_len: 128   # Shorter questions
  plan_len: 384       # Longer plans
  exec_len: 512       # Keep execution the same
  total_len: 1024
```

### 3. Thay Ä‘á»•i block size

Block size áº£nh hÆ°á»Ÿng Ä‘áº¿n trade-off giá»¯a cháº¥t lÆ°á»£ng vÃ  tá»‘c Ä‘á»™:
- `block_size=1`: Autoregressive (cháº­m nháº¥t, cháº¥t lÆ°á»£ng cao nháº¥t)
- `block_size=4,8,16`: BD3-LM (cÃ¢n báº±ng)
- `block_size=1024`: Full diffusion (nhanh nháº¥t, cháº¥t lÆ°á»£ng tháº¥p hÆ¡n)

```bash
# Trong training script
BLOCK_SIZE=8  # hoáº·c 4, 16, 32, ...
```

## Troubleshooting

### Lá»—i: "Mask dimensions don't match"

Kiá»ƒm tra tá»•ng Ä‘á»™ dÃ i:
```python
total = question_len + plan_len*2 + exec_len*2  # *2 vÃ¬ cÃ³ xt vÃ  x0
assert total == expected_mask_size
```

### Lá»—i: "Unknown attention backend"

Äáº£m báº£o báº¡n cÃ i Ä‘áº·t Ä‘Ãºng dependencies:
```bash
pip install torch>=2.0  # Cáº§n cho SDPA
# Hoáº·c cho FlexAttention (tÃ¹y chá»n):
pip install flash-attn==2.5.6
```

### Model khÃ´ng há»c Ä‘Æ°á»£c

1. Kiá»ƒm tra attention mask cÃ³ Ä‘Ãºng khÃ´ng (dÃ¹ng visualization á»Ÿ trÃªn)
2. Giáº£m learning rate: `optim.lr=1e-4`
3. TÄƒng warmup steps: `training.warmup_steps=20000`
4. Check data quality: In ra vÃ i samples Ä‘á»ƒ xem format cÃ³ Ä‘Ãºng khÃ´ng

## So sÃ¡nh vá»›i baseline

| Model | Block Size | Speed | Quality | Hierarchical |
|-------|-----------|-------|---------|--------------|
| AR | 1 | â­ | â­â­â­â­â­ | âŒ |
| MDLM | 1024 | â­â­â­â­â­ | â­â­ | âŒ |
| BD3-LM | 16 | â­â­â­ | â­â­â­â­ | âŒ |
| **Hier-BD3-LM** | 16 | â­â­â­ | â­â­â­â­ | âœ… |

## TÃ­nh nÄƒng Ä‘Ã£ táº¯t (Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a)

Theo yÃªu cáº§u cá»§a báº¡n, cÃ¡c tÃ­nh nÄƒng sau Ä‘Ã£ Ä‘Æ°á»£c táº¯t/Ä‘Æ¡n giáº£n hÃ³a:

1. âŒ **Arbitrary-length generation**: Fixed length (1024 tokens)
2. âŒ **Variable-length sampling**: Disabled by default
3. âœ… **KV caching**: Enabled Ä‘á»ƒ tÄƒng tá»‘c
4. âœ… **First-hitting sampler**: Enabled (faster than DDPM)

Náº¿u muá»‘n báº­t láº¡i, sá»­a trong config:
```yaml
sampling:
  var_length: true
  first_hitting: false
```

## Tham kháº£o Paper

Appendix liÃªn quan:
- **Appendix B.6**: Block diffusion attention mask design
- **Appendix B.7**: Hierarchical reasoning architecture
- **Figure 4**: Visualization of attention patterns

## Citation

Náº¿u sá»­ dá»¥ng code nÃ y, vui lÃ²ng cite paper gá»‘c:
```bibtex
@inproceedings{arriola2025block,
  title={Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models},
  author={Arriola, Marianne and Gokaslan, Aaron and Chiu, Justin T and Yang, Zhihan and Qi, Zhixuan and Han, Jiaqi and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  booktitle={ICLR},
  year={2025}
}
```

## LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c gáº·p váº¥n Ä‘á», vui lÃ²ng:
1. Check documentation á»Ÿ trÃªn
2. Xem code examples trong `hierarchical_dataloader.py`
3. Test vá»›i data nhá» trÆ°á»›c (1000 examples)
4. Visualize attention mask Ä‘á»ƒ debug

Good luck vá»›i research! ğŸš€
